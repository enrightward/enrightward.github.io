{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Cross entropy — Part 2: Convexity of the objective function\"\n",
    "date: 2021-02-24\n",
    "categories: [statistics, entropy]\n",
    "tags: [statistics, cross entropy, softmax, convexity]\n",
    "preview_image: /enrightward.github.io/assets/img/cross-entropy/convex.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Desktop View](/assets/img/cross-entropy/convex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In the [previous post](https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/), I recalled the definition of the _cross entropy_ $H(p, q)$ of two discrete PDFs $p$ and $q$ over the same support $S = \\{ x_{1}, \\ldots, x_{n} \\}$. It is a loose measure of similarity of $p$ and $q$, and so is used in machine learning to define objective functions for tasks where the goal is to learn a PDF $p$ implicit in training data by updating the internal parameters of a learnt PDF $q$. I also wrote down a proof that for fixed but arbitrary $p$, the function $q \\mapsto H(p, q)$ obtains a global minimum at $q = p$.\n",
    "\n",
    "In this post, I will show that $q \\mapsto H(p, q)$ is a convex function, provided we restrict $p$ and $q$ to the space of all PDFs such that no event in $S$ is impossible, i.e. we assume the $p(x_{i})$ and $q(x_{i})$ are never zero. This assumption is not very restrictive for machine learning purposes, where learnt zero probabilities are rare. It also allows us to use the _softmax_ parametrisation for $q$, explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition of cross entropy\n",
    "\n",
    "Recall that the _cross entropy_ of a pair $(p, q)$ of discrete PDFs with the same support $S$ is defined to be:\n",
    "\n",
    "\\begin{equation}\n",
    "H(p, q) := -\\sum_{x \\in S}^{n} p(x) \\log(q(x)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The softmax parametrisation of a discrete PDF\n",
    "\n",
    "For any collection $t_{1}, \\ldots, t_{n}$ of real numbers, the $n$-tuple:\n",
    "\n",
    "\\begin{equation}\n",
    "q(t_{1}, \\ldots, t_{n}) := \\left( \\frac{e^{t_{1}}}{Z}, \\ldots, \\frac{e^{t_{n}}}{Z} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $Z$ is the normalisation constant $\\sum_{i=1}^{n} e^{t_{i}}$, is a PDF. Indeed, each entry of $q$ is positive-valued, since its numerator and denominator are sums of real-valued exponentials, and these entries sum to $1$, by construction of $Z$. Conversely, any PDF $q = (q_{1}, \\ldots, q_{n})$ with all non-zero entries can be written in this form: Simply set $t_{i}$ equal to $\\log(q_{i})$. We call this the _softmax parametrisation_ of $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Proof that $q \\mapsto H(p, q)$ is convex\n",
    "\n",
    "We showed in the previous post that the function $q \\mapsto H(p, q)$, with $p$ fixed and $q$ varying, has a global minimum at $q = p$. We now show that this is a convex function, assuming $p$ and $q$ have no zero entries. By the above assumption, we can write $q$ using the softmax parametrisation:\n",
    "\n",
    "\\begin{equation}\n",
    "q(x_{j}) = \\frac{e^{t_{j}}}{Z},\n",
    "\\end{equation}\n",
    "\n",
    "where $Z$ is the normalisation constant $\\sum_{i=1}^{n} e^{t_{i}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Use the softmax parametrisation\n",
    "\n",
    "Fix $p$ and re-write $H(p, q)$ by replacing $q(x_{i})$ with $e^{t_{i}}/Z$:\n",
    "\n",
    "\\begin{equation}\n",
    "H(p, q) = -\\sum_{i=1}^{n} p(x_{i}) \\log(q(x_{i})) = -\\sum_{i=1}^{n} p(x_{i}) \\log \\left( \\frac{e^{t_{i}}}{Z} \\right) = \n",
    "\\sum_{i=1}^{n} p(x_{i})(\\log(Z) - t_{i}) = \\log(Z) - \\sum_{i=1}^{n} p(x_{i}) t_{i}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute first derivative\n",
    "\n",
    "Now we find a local optimum for $H(p, q)$, regarded as a function of $q$ for fixed but arbitrary $p$. The first step is to solve $\\partial H/\\partial t_{j} = 0$. To do this, observe that because $Z = \\sum_{i=1}^{n} e^{t_{i}}$, we have $\\partial Z/\\partial t_{j} = e^{t_{j}}$, so that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\log(Z)}{t_{j}} = \\frac{e^{t_{j}}}{Z} = q(x_{j}),\n",
    "\\end{equation}\n",
    "\n",
    "and hence:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial H(p, q)}{\\partial t_{j}} = q(x_{j}) - p(x_{j}).\n",
    "\\end{equation}\n",
    "\n",
    "This partial derivative is zero exactly when $q(x_{j}) = p(x_{j})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Hessian is positive definite\n",
    "\n",
    "The previous step implies that for fixed $p$, the quantity $H(p, q)$ is locally flat around $q = p$. To show this is a global minimum, we will show $H$ is \"concave up\" as a function in $q$. It suffices to show that the Hessian matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla^{2} H(p, q) = \\left( \\frac{\\partial^{2} H(p, q)}{\\partial t_{j} \\partial t_{i}} \\right)_{i, j=1}^{n}\n",
    "\\end{equation}\n",
    "\n",
    "is positive definite, i.e. satisfies $v^{t} \\nabla^{2} H(p, q) v \\ge 0$ for all $v \\in \\mathbb{R}^{n}$, with equality only for $v = 0$. First let's compute the entries of the Hessian. From the partial derivative calculation in the previous step, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial^{2} H(p, q)}{\\partial t_{j} \\partial t_{i}} = \\frac{\\partial}{\\partial t_{j}}(q(x_{i}) - p(x_{i})) = \n",
    "\\frac{\\partial q(x_{i})}{\\partial t_{j}},\n",
    "\\end{equation}\n",
    "\n",
    "because $p$ is constant. Now, \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial q(x_{i})}{\\partial t_{j}} = \\frac{\\partial}{\\partial t_{j}} \\left( \\frac{e^{t_{i}}}{Z} \\right) = \n",
    "\\frac{Z \\cdot \\frac{\\partial e^{t_{i}}}{\\partial t_{j}} - e^{t_{i}} \\cdot \\frac{\\partial Z}{\\partial t_{j}}}{Z^{2}} = \n",
    "\\frac{Z \\cdot \\frac{\\partial e^{t_{i}}}{\\partial t_{j}} - e^{t_{i} + t_{j}}}{Z^{2}},\n",
    "\\end{equation}\n",
    "noting that $\\frac{\\partial Z}{\\partial t_{j}} = e^{t_{j}}$ from Step 2. On the other hand, the quantity $\\frac{\\partial e^{t_{i}}}{\\partial t_{j}}$ is equal either to zero, if $i$ and $j$ differ, or else $e^{t_{i}}$, if $i$ and $j$ are the same. It follows that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial^{2} H(p, q)}{\\partial t_{j} \\partial t_{i}} = -\\frac{e^{t_{i} + t_{j}}}{Z^{2}} = -q(x_{i})q(x_{j}),\n",
    "\\end{equation}\n",
    "\n",
    "if $i \\neq j$, and\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial^{2} H(p, q)}{\\partial t_{i}^2{}} = \\frac{Z e^{t_{i}} - e^{2t_{i}}}{Z^{2}} = q(x_{i}) - q(x_{i})^{2}.\n",
    "\\end{equation}\n",
    "\n",
    "The Hessian can thus be written:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla^{2} H(p, q) = \\textrm{diag}(Q) - QQ^{t},\n",
    "\\end{equation}\n",
    "\n",
    "where $Q = (q(x_{1}), \\ldots, q(x_{n}))$ is an $n$-dimensional column vector, and $\\textrm{diag}(Q)$ is the $n \\times n$ matrix whose diagonal is defined by $Q$, and whose off-diagonal entries are zero. Observe now that for $v \\in \\mathbb{R}^{n}$, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "v \\nabla^{2} H(p, q) v^{t} = v \\, \\textrm{diag}(Q) v^{t} - v QQ^{t} v^{t} = \\\\\n",
    "\\sum_{i=1}^{n} v_{i}^{2} q(x_{i}) - \\sum_{i=1}^{n} v_{i}^{2} q(x_{i})^{2} =\n",
    "\\sum_{i=1}^{n} v_{i}^{2} q(x_{i}) (1 - q(x_{i})).\n",
    "\\end{equation}\n",
    "Here, each summand $v_{i}^{2} q(x_{i}) (1 - q(x_{i}))$ is non-negative, being the product of a square, a probability and its complementary probability. It follows that the whole sum is non-negative. Since $p$ and $q$ have no zero probabilities by assumption, this sum can be zero only if each $v_{i}$ is zero, so $\\nabla^{2} H(p, q)$ is positive definite and the minimum $q = p$ is global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Roundup \n",
    "\n",
    "We introduced the softmax function, which can differentiably paramatrise any discrete PDF with no zero probabilities. Then, using the softmax parametrisation, we showed that the $p \\mapsto H(p, q)$ is a convex function. This convexity is important because, as noted in the [previous post](https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/), a common machine learning task is to update a parametrised PDF $q$ to more closely resemble an idealised PDF $p$ implicit in some data set — language modelling often uses this technique, for example. The above convexity result, combined with last post's proof that $q \\mapsto H(p, q)$ achieves a minimum at $q = p$, implies this function can be optimised using a gradient descent algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
