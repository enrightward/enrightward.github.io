[ { "title": "Convolutions of RVs — Part 2: A linear combination of uniform RVs", "url": "/enrightward.github.io/posts/convolutions-part-2/", "categories": "statistics, convolutions", "tags": "statistics, convolutions, uniform distribution, linear combinations random variables", "date": "2021-03-16 00:00:00 +0100", "snippet": "1. IntroductionIn the last post, we defined the convolution of the PDFs \\(f_{X}\\) and \\(f_{Y}\\) of two independent, continuous random variables \\(X\\) and \\(Y\\), and showed that it computed the PDF \\(f_{X + Y}\\) of the sum of \\(X+Y\\). Minor adaptations also yielded a method for computing \\(f_{Y - ..." }, { "title": "Convolutions of random variables — Part 1: Introduction", "url": "/enrightward.github.io/posts/convolutions-part-1/", "categories": "statistics, convolutions", "tags": "statistics, convolutions, uniform distribution, sums of random variables", "date": "2021-03-14 00:00:00 +0100", "snippet": "1. IntroductionSuppose that \\(X\\) and \\(Y\\) are two continuous random variables. Given their PDFs PDFs \\(f_{X}\\) and \\(f_{Y}\\), how do you compute the \\(f_{X + Y}\\) of the sum of \\(X+Y\\)? In this post, we’ll define, compute examples of and prove some facts about the convolution \\(f_{X} * f_{Y}\\) ..." }, { "title": "Order statistics — Part 3: Proof of expectation formula for uniform RVs", "url": "/enrightward.github.io/posts/order-statistics-part-3/", "categories": "statistics, order statistics", "tags": "statistics, order statistics, uniform distribution, expectation formula, gamma function, beta function", "date": "2021-03-08 00:00:00 +0100", "snippet": "1. IntroductionIn the last post, we proved the following general formulae, after conducting some numerical experiments to gain intuition:\\begin{align}F_{X_{(k)}}(x) &amp;amp;= \\sum_{j=k}^{n} \\binom{n}{j} F_{X}(x)^{j} (1 - F_{X}(x))^{n-j}, \\newlinef_{X_{(k)}}(x) &amp;amp;= k \\binom{n}{k} f_{X}(x) ..." }, { "title": "Order statistics — Part 2: General formulae for PDFs and expectations", "url": "/enrightward.github.io/posts/order-statistics-part-2/", "categories": "statistics, order statistics", "tags": "statistics, order statistics, uniform distribution, general formula", "date": "2021-03-07 00:00:00 +0100", "snippet": "1. IntroductionIn the last post, we defined the order statistics of a collection of iid random variables \\(X_{1}, \\ldots, X_{n}\\), to try and answer questions like “What’s the expected value of \\(\\max( \\{ X_{1}, \\ldots, X_{n} \\} )\\)?”, or more generally “what is \\(\\mathbb{E}[(X_{(k)})]\\)?”, where..." }, { "title": "Order statistics — Part 1: Introduction and definitions", "url": "/enrightward.github.io/posts/order-statistics-part-1/", "categories": "statistics, order statistics", "tags": "statistics, order statistics, uniform distribution, FTC", "date": "2021-03-05 00:00:00 +0100", "snippet": "1. IntroductionOn average, how tall is the tallest 8th grader in a class of 30 students, in the US? What is the value of the average winning bid in a Vickery Auction, where the highest bidder wins, but must pay only the second-highest bid? A common statistical approach here is: Assume the empiri..." }, { "title": "Cross entropy — Part 2: Convexity of the objective function", "url": "/enrightward.github.io/posts/cross-entropy-part-2/", "categories": "statistics, entropy", "tags": "statistics, cross entropy, softmax, convexity", "date": "2021-02-24 00:00:00 +0100", "snippet": "1. IntroductionIn the previous post, I recalled the definition of the cross entropy \\(H(p, q)\\) of two discrete PDFs \\(p\\) and \\(q\\) over the same support \\(S = \\{ x_{1}, \\ldots, x_{n} \\}\\). It is a loose measure of similarity of \\(p\\) and \\(q\\), and so is used in machine learning to define objec..." }, { "title": "Cross entropy — Part 1: Introduction and definitions", "url": "/enrightward.github.io/posts/cross-entropy-part-1/", "categories": "statistics, entropy", "tags": "statistics, entropy, cross entropy", "date": "2021-02-23 00:00:00 +0100", "snippet": "1. IntroductionIn this post, I discuss the cross entropy \\(H(p, q)\\) of two discrete PDFs \\(p\\) and \\(q\\). It is a loose measure of similarity of \\(p\\) and \\(q\\), and so is used in machine learning to define objective functions for tasks where the goal is to learn a PDF \\(p\\) implicit in training..." } ]
