<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Cross entropy — Part 1: Introduction and definitions" /><meta name="author" content="Stephen Enright-Ward" /><meta property="og:locale" content="en_US" /><meta name="description" content="A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation." /><meta property="og:description" content="A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation." /><link rel="canonical" href="https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/" /><meta property="og:url" content="https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/" /><meta property="og:site_name" content="Logical Quantifier" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-02-23T00:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Cross entropy — Part 1: Introduction and definitions" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@Stephen Enright-Ward" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation.","url":"https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/","@type":"BlogPosting","headline":"Cross entropy — Part 1: Introduction and definitions","dateModified":"2021-03-11T09:00:54+01:00","datePublished":"2021-02-23T00:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/"},"author":{"@type":"Person","name":"Stephen Enright-Ward"},"@context":"https://schema.org"}</script><title>Cross entropy — Part 1: Introduction and definitions | Logical Quantifier</title><link rel="shortcut icon" href="/enrightward.github.io/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/enrightward.github.io/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/enrightward.github.io/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/enrightward.github.io/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/enrightward.github.io/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/enrightward.github.io/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/enrightward.github.io/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/enrightward.github.io/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/enrightward.github.io/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/enrightward.github.io/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/enrightward.github.io/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/enrightward.github.io/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/enrightward.github.io/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/enrightward.github.io/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/enrightward.github.io/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/enrightward.github.io/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/enrightward.github.io/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/enrightward.github.io/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/enrightward.github.io/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/enrightward.github.io/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/enrightward.github.io/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/enrightward.github.io/assets/js/dist/post.min.js"></script> <script defer src="/enrightward.github.io/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/enrightward.github.io/" alt="avatar" class="mx-auto"> <img src="https://cdn.jsdelivr.net/gh/enrightward/blog-images/abacus.svg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/enrightward.github.io/">Logical Quantifier</a></div><div class="site-subtitle font-italic">A statistics, machine learning and mathematics blog</div></div><ul class="w-100"><li class="nav-item"> <a href="/enrightward.github.io/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/enrightward.github.io/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/enrightward.github.io/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/enrightward.github.io/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/enrightward.github.io/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/enrightward" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['',''].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/enrightward.github.io/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/enrightward.github.io/"> Posts </a> </span> <span>Cross entropy — Part 1: Introduction and definitions</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Cross entropy — Part 1: Introduction and definitions</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Feb 23, 2021, 12:00 AM +0100" > Feb 23 <i class="unloaded">2021-02-23T00:00:00+01:00</i> </span> by <span class="author"> Stephen Enright-Ward </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Mar 11, 2021, 9:00 AM +0100" > Mar 11 <i class="unloaded">2021-03-11T09:00:54+01:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1327 words">7 min</span></div></div><div class="post-content"><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/enrightward.github.io/assets/img/cross-entropy/blue-white-paint.jpg" alt="Desktop View" /></p><h2 id="1-introduction">1. Introduction</h2><p>In this post, I discuss the <em>cross entropy</em> \(H(p, q)\) of two discrete PDFs \(p\) and \(q\). It is a loose measure of similarity of \(p\) and \(q\), and so is used in machine learning to define objective functions for tasks where the goal is to learn a PDF \(p\) implicit in training data by updating the internal parameters of a learnt PDF \(q\). After explaining this application in more detail, I:</p><ul><li>Recall the definitions of <em>entropy</em> and <em>cross entropy</em> ;<li>Use numpy to do some example computations using cross entropy, and;<li>Write down a proof that for fixed but arbitrary \(p\), the function \(q \mapsto H(p, q)\) obtains a global minimum at \(q = p\).</ul><p>This last point both suggests that cross entropy measures similarity.</p><h2 id="2-why-is-cross-entropy-useful-in-machine-learning">2. Why is cross entropy useful in machine learning?</h2><p>The above-mentioned global minimum \(q = p\) of \(q \mapsto H(p, q)\) suggests that cross entropy is a measure of closeness between \(p\) and \(q\). This is not true in the strict sense of being a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)#Definition">metric à la pure mathematics</a> (in general, \(H(p, p)\) is not zero, and \(H(p, q)\) and \(H(q, p)\) are different), but it is true enough to be useful in machine learning. Before jumping into details, let me give an example of this usefulness.</p><p>In language modelling, the goal is to train the machine to predict the next word in a sentence, given the preceding ones. The sentence might be:</p><blockquote><p>“the cat sat on the [mat]”.</p></blockquote><p>The machine would have to predict “mat”, given the other words. Cross entropy enters the picture when one formalises this task, by defining:</p><ul><li>The PDF \(p\) to be the mythical PDF that always predicts the right answer — formally, it is the “one-hot” PDF \(p\) over all words in the English language defined by \(p(\textrm{mat}) = 1\) and \(p(w) = 0\) for all other words \(w\) — and<li>The PDF \(q\) to be the machine’s best guess about the next word, based on the text it has seen so far.</ul><p>Under the hood, \(q\) often depends on many, many parameters, which the machine can alter. The machine’s learning task is to update these parameters to make \(q\) as “close” to \(p\) as possible. Here, people can and often do define “close” to mean the cross entropy \(H(p, q)\) is as small as possible.</p><h2 id="3-tldr">3. TL;DR:</h2><p>Cross entropy is often used in machine learning to define an objective function that measures the difference between a mythical, always-correct PDF \(p\) (which in practice is approximated from the data), and the machine’s best-guess PDF \(q\).</p><h2 id="4-a-quick-introduction-to-entropy">4. A quick introduction to entropy</h2><p>Suppose \(p\) is a discrete PDF with support (“outcome space”) \(S = \{ x_{1}, \ldots, x_{n} \}\). The <em>entropy</em> of \(p\) is defined to be:</p><p>\begin{equation} H(p) := \sum_{i=1}^{n} p(x_{i}) \log \left( \frac{1}{p(x_{i})} \right) = -\sum_{i=1}^{n} p(x_{i}) \log(p(x_{i})), \end{equation}</p><p>provided none of the \(p(x_{i})\) is zero. If \(p(x_{i}) = 0\) for some \(i\), then we declare by fiat that the summand \(p(x_{i}) \log(p(x_{i}))\) is zero. One justification for this is that:</p><p>\begin{equation} \lim_{x \rightarrow 0^{+}} x \log x = 0. \end{equation}</p><p>The quantity \(s(x_{i}) := \log(1/p(x_{i}))\) is called the <em>surprisal</em> of the outcome \(x_{i}\). It quantifies the “information” associated to \(x_{i}\) — lower probability events are more surprising, hence their occurrence more informative. <a href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem#:~:text=In%20information%20theory%2C%20Shannon's%20source,meaning%20of%20the%20Shannon%20entropy.&amp;text=However%20it%20is%20possible%20to,with%20negligible%20probability%20of%20loss.">Shannon’s Source Coding Theorem</a> states that, if the logarithm is base 2, then \(s(x_{i})\) approximates the length in bits of an optimal, lossless binary encoding for a message written in the outcome space alphabet \(x_{i}\) and whose characters appear in this message with probabilities defined by \(p\).</p><h2 id="5-cross-entropy">5. Cross entropy</h2><p>I won’t talk more about entropy in this blog — instead I discuss <em>cross entropy</em>. Let \(q\) be a second PDF with the same support \(S\). Then the <em>cross entropy</em> of the pair \((p, q)\) is defined to be:</p><p>\begin{equation} H(p, q) := -\sum_{i=1}^{n} p(x_{i}) \log(q(x_{i})). \end{equation}</p><p>This is not a symmetric quantity: \(H(p, q)\) and \(H(q, p)\) are different in general. We’ll see this by computing examples. To do this, we first define two python functions.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="s">"""Require that p and q are numpy arrays"""</span>
    <span class="n">result</span> <span class="o">=</span> <span class="o">-</span><span class="n">p</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">random_discrete_pdf</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
    <span class="s">"""Compute a random multinomial 
    PDF with `length` entries"""</span>
    <span class="c1"># Generate `length` random numbers between 0 and 1.
</span>    <span class="n">pdf</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="c1"># normalise, so it's a pdf
</span>    <span class="n">pdf</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">/</span><span class="n">pdf</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">pdf</span>
</pre></table></code></div></div><p>Taking \(p\) and \(q\) to be two randomly-generated PDFs of length five, we have:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="n">length</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">random_discrete_pdf</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">random_discrete_pdf</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'p ='</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'q ='</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">'H(p, q) = {:0.3f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'H(q, p) = {:0.3f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">)))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>p = [0.08669807 0.24134156 0.32611669 0.16300244 0.18284124]
q = [0.17582746 0.02024274 0.29378282 0.14374014 0.36640684]

H(p, q) = 1.991
H(q, p) = 1.671
</pre></table></code></div></div><p>Now let’s keep \(p\) fixed, and compute \(H(p, q)\) for randomly-generated \(q\):</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">N</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">hpqs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">random_discrete_pdf</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="n">hpq</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
    <span class="n">hpqs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">hpq</span><span class="p">)</span>
    
<span class="n">hpqs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">hpqs</span><span class="p">)</span>
</pre></table></code></div></div><p>We plot the \(H(p, q)\)’s in a histogram, to get a sense of the distribution.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="n">num_bins</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">n</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">hpqs</span><span class="p">,</span> <span class="n">num_bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">colour</span> <span class="o">=</span> <span class="s">'white'</span>

<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'bottom'</span><span class="p">].</span><span class="n">set_color</span><span class="p">(</span><span class="n">colour</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_color</span><span class="p">(</span><span class="n">colour</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'left'</span><span class="p">].</span><span class="n">set_color</span><span class="p">(</span><span class="n">colour</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_color</span><span class="p">(</span><span class="n">colour</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="n">label</span><span class="p">.</span><span class="n">set_color</span><span class="p">(</span><span class="n">colour</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="n">label</span><span class="p">.</span><span class="n">set_color</span><span class="p">(</span><span class="n">colour</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">title</span><span class="p">.</span><span class="n">set_color</span><span class="p">(</span><span class="n">colour</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colour</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="n">colour</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'H(p, q)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Frequency'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s">'Distribution of H(p, q)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="cross-entropy-part-1_files/cross-entropy-part-1_13_0.png" alt="png" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/enrightward.github.io/assets/img/cross-entropy/hpq_histogram.png" alt="png" /></p><p>There appears to be a hard cut-off around \(1 \le H(p, q) \le 2\), below which there are no samples. What’s going on? It turns out this lower bound is \(H(p, p)\). Empirically, this looks correct:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="k">print</span><span class="p">(</span><span class="s">'H(p, p) = {:0.3f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">)))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>H(p, p) = 1.527
</pre></table></code></div></div><h2 id="6-proof-that-q--p-minimises-p-mapsto-hp-q">6. Proof that \(q = p\) minimises \(p \mapsto H(p, q)\)</h2><p>The goal is to show that for a fixed discrete PDF \(p\), \(H(p, p) \le H(p, q)\) for all PDFs \(q\) with the same support as \(p\). We use the following fact: Since \(y = x - 1\) is the tangent at \(x = 1\) to the convex function \(y = \log(x)\), we have:</p><p>\begin{equation} \log(x) \le x - 1, \end{equation}</p><p>for all \(x \ge 0\). To ease notation, write \(p_{i} := p(x_{i})\) for all \(1 \le i \le n\), and define \(I := \{ 1 \le i \le n \mid p_{i} \neq 0 \}\). Then:</p><p>\begin{equation} \sum_{i \in I} p_{i} \log \left( \frac{q_{i}}{p_{i}} \right) \le^{\textrm{(a)}} \sum_{i \in I} p_{i} \left(\frac{q_{i}}{p_{i}} - 1 \right) = <br /> \sum_{i \in I} q_{i} - \sum_{i \in I} p_{i} =^{\textrm{(b)}} \left( \sum_{i \in I} q_{i} \right) - 1 \le^{\textrm{(c)}} 0. \end{equation}</p><p>Here, the inequality (a) follows from the log inequality above, and the equality (b) holds because the sum of \(p_{i}\) over \(I\) is 1. Since the sum of \(q_{i}\) over \(I\) may be less than 1, inequality (c) holds. So we’ve shown that:</p><p>\begin{equation} 0 \ge \sum_{i \in I} p_{i} \log \left( \frac{q_{i}}{p_{i}} \right) = \sum_{i \in I} p_{i} \log (q_{i}) - \sum_{i \in I} p_{i} \log (p_{i}), \end{equation}</p><p>but this implies that:</p><p>\begin{equation} H(p, p) = - \sum_{i \in I} p_{i} \log (p_{i}) \le -\sum_{i \in I} p_{i} \log (q_{i}) = H(p, q). \end{equation}</p><h2 id="7-roundup">7. Roundup</h2><p>We introduced the notion of cross entropy, and noted its usefulness in machine learning as an objective function measuring the difference between an ideal PDF \(p\) implicit in a data set, and a learnt, parametrised PDF \(q\). This has particular applications in language modelling. We then wrote down a proof that for fixed but arbitrary \(p\), the function \(q \mapsto H(p, q)\) obtains a global minimum at \(q = p\). In the <a href="https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-2/">next post</a>, we will show, with the help of the softmax parametrisation, that this function is convex, so can be optimised via a gradient descent algorithm.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/enrightward.github.io/categories/statistics/'>statistics</a>, <a href='/enrightward.github.io/categories/entropy/'>entropy</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/enrightward.github.io/tags/statistics/" class="post-tag no-text-decoration" >statistics</a> <a href="/enrightward.github.io/tags/entropy/" class="post-tag no-text-decoration" >entropy</a> <a href="/enrightward.github.io/tags/cross-entropy/" class="post-tag no-text-decoration" >cross entropy</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Cross entropy — Part 1: Introduction and definitions - Logical Quantifier&url=https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Cross entropy — Part 1: Introduction and definitions - Logical Quantifier&u=https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Cross entropy — Part 1: Introduction and definitions - Logical Quantifier&url=https://enrightward.github.io/enrightward.github.io/posts/cross-entropy-part-1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/enrightward.github.io/posts/convolutions-part-2/">Convolutions of RVs — Part 2: A linear combination of uniform RVs</a><li><a href="/enrightward.github.io/posts/order-statistics-part-3/">Order statistics — Part 3: Proof of expectation formula for uniform RVs</a><li><a href="/enrightward.github.io/posts/cross-entropy-part-2/">Cross entropy — Part 2: Convexity of the objective function</a><li><a href="/enrightward.github.io/posts/cross-entropy-part-1/">Cross entropy — Part 1: Introduction and definitions</a><li><a href="/enrightward.github.io/posts/order-statistics-part-2/">Order statistics — Part 2: General formulae for PDFs and expectations</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/enrightward.github.io/tags/statistics/">statistics</a> <a class="post-tag" href="/enrightward.github.io/tags/uniform-distribution/">uniform distribution</a> <a class="post-tag" href="/enrightward.github.io/tags/order-statistics/">order statistics</a> <a class="post-tag" href="/enrightward.github.io/tags/convolutions/">convolutions</a> <a class="post-tag" href="/enrightward.github.io/tags/cross-entropy/">cross entropy</a> <a class="post-tag" href="/enrightward.github.io/tags/beta-function/">beta function</a> <a class="post-tag" href="/enrightward.github.io/tags/convexity/">convexity</a> <a class="post-tag" href="/enrightward.github.io/tags/entropy/">entropy</a> <a class="post-tag" href="/enrightward.github.io/tags/expectation-formula/">expectation formula</a> <a class="post-tag" href="/enrightward.github.io/tags/ftc/">FTC</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/enrightward.github.io/posts/cross-entropy-part-2/"><div class="card-body"> <span class="timeago small" > Feb 24 <i class="unloaded">2021-02-24T00:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Cross entropy — Part 2: Convexity of the objective function</h3><div class="text-muted small"><p> 1. Introduction In the previous post, I recalled the definition of the cross entropy \(H(p, q)\) of two discrete PDFs \(p\) and \(q\) over the same support \(S = \{ x_{1}, \ldots, x_{n} \}\). It...</p></div></div></a></div><div class="card"> <a href="/enrightward.github.io/posts/order-statistics-part-1/"><div class="card-body"> <span class="timeago small" > Mar 5 <i class="unloaded">2021-03-05T00:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Order statistics — Part 1: Introduction and definitions</h3><div class="text-muted small"><p> 1. Introduction On average, how tall is the tallest 8th grader in a class of 30 students, in the US? What is the value of the average winning bid in a Vickery Auction, where the highest bidder w...</p></div></div></a></div><div class="card"> <a href="/enrightward.github.io/posts/order-statistics-part-2/"><div class="card-body"> <span class="timeago small" > Mar 7 <i class="unloaded">2021-03-07T00:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Order statistics — Part 2: General formulae for PDFs and expectations</h3><div class="text-muted small"><p> 1. Introduction In the last post, we defined the order statistics of a collection of iid random variables \(X_{1}, \ldots, X_{n}\), to try and answer questions like “What’s the expected value of...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <span class="btn btn-outline-primary disabled" prompt="Older"><p>-</p></span> <a href="/enrightward.github.io/posts/cross-entropy-part-2/" class="btn btn-outline-primary" prompt="Newer"><p>Cross entropy — Part 2: Convexity of the objective function</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://twitter.com/username">Stephen Enright-Ward</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/enrightward.github.io/tags/statistics/">statistics</a> <a class="post-tag" href="/enrightward.github.io/tags/uniform-distribution/">uniform distribution</a> <a class="post-tag" href="/enrightward.github.io/tags/order-statistics/">order statistics</a> <a class="post-tag" href="/enrightward.github.io/tags/convolutions/">convolutions</a> <a class="post-tag" href="/enrightward.github.io/tags/cross-entropy/">cross entropy</a> <a class="post-tag" href="/enrightward.github.io/tags/beta-function/">beta function</a> <a class="post-tag" href="/enrightward.github.io/tags/convexity/">convexity</a> <a class="post-tag" href="/enrightward.github.io/tags/entropy/">entropy</a> <a class="post-tag" href="/enrightward.github.io/tags/expectation-formula/">expectation formula</a> <a class="post-tag" href="/enrightward.github.io/tags/ftc/">FTC</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/enrightward.github.io/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://enrightward.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
